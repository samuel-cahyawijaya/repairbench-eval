{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9444b324-1333-490f-b84d-254d56f3f12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "import json\n",
    "import jsonlines\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import requests\n",
    "import base64\n",
    "import pickle\n",
    "import torch\n",
    "\n",
    "from FlagEmbedding import BGEM3FlagModel\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aef7ba50-5b34-4c92-a70d-7b6e5c893f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading `repairbench_ko` with 55 data points.\n",
      "Loaded 162 document chunks for `repairbench_ko`\n",
      "\n",
      "Loading `repairbench_en` with 373 data points.\n",
      "Loaded 2224 document chunks for `repairbench_en`\n",
      "\n",
      "Loading `repairbench_ja` with 222 data points.\n",
      "Loaded 1429 document chunks for `repairbench_ja`\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read raw data\n",
    "test_data_dict = {}\n",
    "docs_data_dict = {}\n",
    "for path in glob('./annotated_data/*.json'):\n",
    "    dset = pd.read_json(path)\n",
    "    dset_name = path.split('/')[-1].replace('.json', '')\n",
    "    test_data_dict[dset_name] = dset\n",
    "    print(f\"Loading `{dset_name}` with {len(dset)} data points.\")\n",
    "\n",
    "    # Read context documents\n",
    "    docs_data = []\n",
    "    for path in glob(f'./repairbench_docs/{dset_name}_docs/*'):\n",
    "        json_data = json.load(open(path, 'r'))\n",
    "        for i in range(len(json_data)):\n",
    "            json_data[i]['metadata']['chunk_idx'] = i+1\n",
    "        docs_data += json_data\n",
    "\n",
    "    docs_data_dict[dset_name] = docs_data\n",
    "    print(f'Loaded {len(docs_data)} document chunks for `{dset_name}`')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6ad3f4-987a-4a46-b0ad-4f3873904bbc",
   "metadata": {},
   "source": [
    "# Retrieval Evaluation BGE-M3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ef7bfab-f266-43c0-b291-1ad1b72aa812",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1287eb5921b448f8869892b76ca3c39f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 30 files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = BGEM3FlagModel('BAAI/bge-m3', use_fp16=False, device=\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6af647c-df14-471e-a334-532501422735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running document embed for `repairbench_ko`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running document embed for `repairbench_en`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pre tokenize: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 35/35 [00:00<00:00, 107.85it/s]\n",
      "Inference Embeddings: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 35/35 [00:35<00:00,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running document embed for `repairbench_ja`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pre tokenize: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 23/23 [00:00<00:00, 117.62it/s]\n",
      "Inference Embeddings:   5%|█████▌                                                                                                          | 2/40 [00:18<05:18,  8.38s/it]"
     ]
    }
   ],
   "source": [
    "# Build Index\n",
    "docs_embed_dict = {}\n",
    "for dset_name, docs_data in docs_data_dict.items():\n",
    "    if os.path.exists(f'./{dset_name}_docs_bge_m3_embed.pkl'):\n",
    "        print(f'Loading `{dset_name}` docs embed from cache...')\n",
    "        docs_embed_dict[dset_name] = pickle.load(open(f'./{dset_name}_docs_bge_m3_embed.pkl', 'rb'))\n",
    "    else:\n",
    "        print(f'Running document embed for `{dset_name}`')\n",
    "        texts = list(map(lambda x: x['text'], docs_data))\n",
    "        docs_embeds = model.encode(texts, batch_size=64, max_length=8192)['dense_vecs']        \n",
    "        pickle.dump(docs_embeds, open(f'{dset_name}_docs_bge_m3_embed.pkl', 'wb'))\n",
    "    docs_embed_dict[dset_name] = docs_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2243f9e3-7e91-4cc2-8930-296f31538303",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_embed_dict = {}\n",
    "for dset_name, test_df in test_data_dict.items():\n",
    "    # Embed all queries\n",
    "    if os.path.exists(f'{dset_name}_query_bge_m3_embed.pkl'):\n",
    "        print(f'Loading `{dset_name}` query embed from cache...')\n",
    "        query_embeds = pickle.load(open(f'{dset_name}_query_bge_m3_embed.pkl', 'rb'))\n",
    "    else:\n",
    "        print(f'Running query embed for `{dset_name}`')\n",
    "        texts = test_df.apply(lambda x: x['data']['question'], axis='columns').tolist()\n",
    "        query_embeds = model.encode(texts, batch_size=8, max_length=8192)['dense_vecs']\n",
    "\n",
    "        pickle.dump(query_embeds, open(f'{dset_name}_query_bge_m3_embed.pkl', 'wb'))\n",
    "    query_embed_dict[dset_name] = query_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abf81ea-4573-4d82-9e7c-8dd0e7f33aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieval Evaluation Params\n",
    "# test_data_dict: Dict<dataset_name, annotated_data_df>\n",
    "# docs_data_dict: Dict<dataset_name, List<document_chunk>>\n",
    "# docs_embed_dict: Dict<dataset_name, Tensor[n_docs, n_dim] >\n",
    "# query_embed_dict: Dict<dataset_name, Tensor[n_queries, n_dim]>\n",
    "\n",
    "ks = np.arange(5, 150, 5)\n",
    "eval_dict = {}\n",
    "for dset_name, test_df in test_data_dict.items():\n",
    "    docs_uids = list(map(lambda x: f\"{x['metadata']['filename']}_{x['metadata']['chunk_idx']}\", docs_data_dict[dset_name]))\n",
    "    silver_uids_list = test_df['data'].apply(lambda x: [f\"{res['title']}_{res['chunk_idx']}\" for res in x['search_results'][0] if res['is_relevant'] in ['1', '2']]).tolist()\n",
    "    gold_uids_list = test_df['data'].apply(lambda x: [f\"{res['title']}_{res['chunk_idx']}\" for res in x['search_results'][0] if res['is_relevant'] in ['2']]).tolist()\n",
    "    \n",
    "    docs_embed = torch.from_numpy(docs_embed_dict[dset_name])\n",
    "    query_embed = torch.from_numpy(query_embed_dict[dset_name])\n",
    "    docs_indices = (query_embed @ docs_embed.T).topk(max(ks), dim=-1).indices\n",
    "\n",
    "    metrics = {}\n",
    "    for k in ks:\n",
    "        k_metrics = defaultdict(lambda: 0)\n",
    "        for silver_uids, gold_uids, docs_idx in zip(silver_uids_list, gold_uids_list, docs_indices):\n",
    "            if len(silver_uids) > 0:\n",
    "                pred_uids = [docs_uids[idx] for idx in docs_idx[:k]]\n",
    "                k_metrics[f'silver_correct'] += len(set(pred_uids).intersection(set(silver_uids)))\n",
    "                k_metrics[f'silver_count'] += len(set(silver_uids))\n",
    "            if len(gold_uids) > 0:\n",
    "                pred_uids = [docs_uids[idx] for idx in docs_idx[:k]]\n",
    "                k_metrics[f'gold_correct'] += len(set(pred_uids).intersection(set(gold_uids)))\n",
    "                k_metrics[f'gold_count'] += len(set(gold_uids))\n",
    "        metrics[k] = {\n",
    "            'silver_acc': k_metrics[f'silver_correct'] / k_metrics[f'silver_count'], \n",
    "            'gold_acc': k_metrics[f'gold_correct'] /k_metrics[f'gold_count'],\n",
    "        }\n",
    "    eval_dict[dset_name] = dict(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ef4e7a-1cf7-4839-a6fb-d883f677508d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_name = 'repairbench_ko'\n",
    "df = pd.DataFrame(eval_dict[dset_name]).reset_index().melt(id_vars='index')\n",
    "sns.lineplot(data=df, x='variable', y='value', hue='index')\n",
    "plt.title(f'Retrieval Eval `{dset_name}`')\n",
    "plt.xlabel('#Search Document')\n",
    "plt.ylabel('Acc@k')\n",
    "plt.axvline(x=15, ymin=0, ymax=1, color='red', linestyle='--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d9505b-ae18-43ec-b3c2-751b505af3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_name = 'repairbench_ja'\n",
    "df = pd.DataFrame(eval_dict[dset_name]).reset_index().melt(id_vars='index')\n",
    "sns.lineplot(data=df, x='variable', y='value', hue='index')\n",
    "plt.title(f'Retrieval Eval `{dset_name}`')\n",
    "plt.xlabel('#Search Document')\n",
    "plt.ylabel('Acc@k')\n",
    "plt.axvline(x=15, ymin=0, ymax=1, color='red', linestyle='--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78b3c8f-e36a-4bf6-88fe-187f4a508d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_name = 'repairbench_en'\n",
    "df = pd.DataFrame(eval_dict[dset_name]).reset_index().melt(id_vars='index')\n",
    "sns.lineplot(data=df, x='variable', y='value', hue='index')\n",
    "plt.title(f'Retrieval Eval `{dset_name}`')\n",
    "plt.xlabel('#Search Document')\n",
    "plt.ylabel('Acc@k')\n",
    "plt.axvline(x=15, ymin=0, ymax=1, color='red', linestyle='--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2699331-2b8e-4089-84fb-6c7726535f91",
   "metadata": {},
   "source": [
    "# Retrieval Evaluation E5-Large Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6013c170-b06a-4f31-a97c-9c45f69e6599",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def get_detailed_instruct(task_description: str, query: str) -> str:\n",
    "    return f'Instruct: {task_description}\\nQuery: {query}'\n",
    "\n",
    "task = 'Given a web search query, retrieve relevant passages that answer the query'\n",
    "model = SentenceTransformer('intfloat/multilingual-e5-large-instruct', device='cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200c04c0-59f0-4934-9ee0-6654608c88d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Index\n",
    "docs_embed_dict = {}\n",
    "for dset_name, docs_data in docs_data_dict.items():\n",
    "    if os.path.exists(f'./{dset_name}_e5_large_instruct_docs_embed.pkl'):\n",
    "        print(f'Loading `{dset_name}` docs embed from cache...')\n",
    "        docs_embed_dict[dset_name] = pickle.load(open(f'./{dset_name}_e5_large_instruct_docs_embed.pkl', 'rb'))\n",
    "    else:\n",
    "        print(f'Running document embed for `{dset_name}`')\n",
    "        texts = list(map(lambda x: get_detailed_instruct(task, x['text']), docs_data))\n",
    "        docs_embeds = model.encode(texts, batch_size=64, convert_to_tensor=True, normalize_embeddings=True)\n",
    "        pickle.dump(docs_embeds, open(f'./{dset_name}_e5_large_instruct_docs_embed.pkl', 'wb'))\n",
    "        \n",
    "        docs_embed_dict[dset_name] = docs_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d0668c-b8a8-49d4-96c3-a31d423b03f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_embed_dict = {}\n",
    "for dset_name, test_df in test_data_dict.items():\n",
    "    # Embed all queries\n",
    "    if os.path.exists(f'{dset_name}_e5_large_instruct_query_embed.pkl'):\n",
    "        print(f'Loading `{dset_name}` query embed from cache...')\n",
    "        query_embeds = pickle.load(open(f'{dset_name}_e5_large_instruct_query_embed.pkl', 'rb'))\n",
    "    else:\n",
    "        print(f'Running query embed for `{dset_name}`')\n",
    "        texts = test_df.apply(lambda x: get_detailed_instruct(task, x['data']['question']), axis='columns').tolist()\n",
    "        query_embeds = model.encode(texts, batch_size=64, convert_to_tensor=True, normalize_embeddings=True)\n",
    "        pickle.dump(query_embeds, open(f'{dset_name}_e5_large_instruct_query_embed.pkl', 'wb'))\n",
    "    query_embed_dict[dset_name] = query_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a39f5a0-159c-4fcc-b604-3b501b7ff71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieval Evaluation Params\n",
    "# test_data_dict: Dict<dataset_name, annotated_data_df>\n",
    "# docs_data_dict: Dict<dataset_name, List<document_chunk>>\n",
    "# docs_embed_dict: Dict<dataset_name, Tensor[n_docs, n_dim] >\n",
    "# query_embed_dict: Dict<dataset_name, Tensor[n_queries, n_dim]>\n",
    "\n",
    "ks = np.arange(5, 150, 5)\n",
    "eval_dict = {}\n",
    "for dset_name, test_df in test_data_dict.items():\n",
    "    docs_uids = list(map(lambda x: f\"{x['metadata']['filename']}_{x['metadata']['chunk_idx']}\", docs_data_dict[dset_name]))\n",
    "    silver_uids_list = test_df['data'].apply(lambda x: [f\"{res['title']}_{res['chunk_idx']}\" for res in x['search_results'][0] if res['is_relevant'] in ['1', '2']]).tolist()\n",
    "    gold_uids_list = test_df['data'].apply(lambda x: [f\"{res['title']}_{res['chunk_idx']}\" for res in x['search_results'][0] if res['is_relevant'] in ['2']]).tolist()\n",
    "    \n",
    "    docs_embed = docs_embed_dict[dset_name]\n",
    "    query_embed = query_embed_dict[dset_name]\n",
    "    docs_indices = (query_embed @ docs_embed.T).topk(max(ks), dim=-1).indices\n",
    "\n",
    "    metrics = {}\n",
    "    for k in ks:\n",
    "        k_metrics = defaultdict(lambda: 0)\n",
    "        for silver_uids, gold_uids, docs_idx in zip(silver_uids_list, gold_uids_list, docs_indices):\n",
    "            if len(silver_uids) > 0:\n",
    "                pred_uids = [docs_uids[idx] for idx in docs_idx[:k]]\n",
    "                k_metrics[f'silver_correct'] += len(set(pred_uids).intersection(set(silver_uids)))\n",
    "                k_metrics[f'silver_count'] += len(set(silver_uids))\n",
    "            if len(gold_uids) > 0:\n",
    "                pred_uids = [docs_uids[idx] for idx in docs_idx[:k]]\n",
    "                k_metrics[f'gold_correct'] += len(set(pred_uids).intersection(set(gold_uids)))\n",
    "                k_metrics[f'gold_count'] += len(set(gold_uids))\n",
    "        metrics[k] = {\n",
    "            'silver_acc': k_metrics[f'silver_correct'] / k_metrics[f'silver_count'], \n",
    "            'gold_acc': k_metrics[f'gold_correct'] /k_metrics[f'gold_count'],\n",
    "        }\n",
    "    eval_dict[dset_name] = dict(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0dcc9af-df2f-4628-bbdb-d266295909ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_name = 'repairbench_ko'\n",
    "df = pd.DataFrame(eval_dict[dset_name]).reset_index().melt(id_vars='index')\n",
    "sns.lineplot(data=df, x='variable', y='value', hue='index')\n",
    "plt.title(f'Retrieval Eval `{dset_name}`')\n",
    "plt.xlabel('#Search Document')\n",
    "plt.ylabel('Acc@k')\n",
    "plt.axvline(x=15, ymin=0, ymax=1, color='red', linestyle='--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db23cdf-b2ed-4569-b607-48dc6a1ea780",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_name = 'repairbench_ja'\n",
    "df = pd.DataFrame(eval_dict[dset_name]).reset_index().melt(id_vars='index')\n",
    "sns.lineplot(data=df, x='variable', y='value', hue='index')\n",
    "plt.title(f'Retrieval Eval `{dset_name}`')\n",
    "plt.xlabel('#Search Document')\n",
    "plt.ylabel('Acc@k')\n",
    "plt.axvline(x=15, ymin=0, ymax=1, color='red', linestyle='--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b934a8-cd7f-46cf-b6a2-5c38e6de83e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_name = 'repairbench_en'\n",
    "df = pd.DataFrame(eval_dict[dset_name]).reset_index().melt(id_vars='index')\n",
    "sns.lineplot(data=df, x='variable', y='value', hue='index')\n",
    "plt.title(f'Retrieval Eval `{dset_name}`')\n",
    "plt.xlabel('#Search Document')\n",
    "plt.ylabel('Acc@k')\n",
    "plt.axvline(x=15, ymin=0, ymax=1, color='red', linestyle='--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc15670f-0036-43f6-ae70-1c35931f6207",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (sandbox)",
   "language": "python",
   "name": "sandbox"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
